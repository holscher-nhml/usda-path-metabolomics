{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metabolomics Data - Difference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Please see README.md for additional information.\n",
    "\n",
    "- This notebook requires a Jupyter environment and Python 3.7+. Install dependencies via `pip install -r requirements.txt`.\n",
    "\n",
    "- \"Fast mode\" is enabled by default, signifcantly reducing the model selection time by eliminating the cross-validation step. Fast mode can be disabled to produce accurate results. Fast mode was not. used by the authors of the manuscript for the purposes of publication.\n",
    "\n",
    "Author: Aditya Mansharamani, adityam5@illinois.edu\n",
    "\n",
    "This code, along with any accompanying source code files ONLY,\n",
    "are released under the license specified in LICENSE.md."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, imports, & formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import csv\n",
    "import scipy\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# formatting\n",
    "sns.set()\n",
    "params = {\n",
    "    \"legend.fontsize\": \"x-large\",\n",
    "    \"figure.figsize\": (15, 10),\n",
    "    \"axes.labelsize\": \"x-large\",\n",
    "    \"axes.titlesize\": \"x-large\",\n",
    "    \"xtick.labelsize\": \"x-large\",\n",
    "    \"ytick.labelsize\": \"x-large\",\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "# turn off spammy warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loads data files\n",
    "def load_data(foods=None, split_grains=False, verbose=False):\n",
    "\n",
    "    # Load files\n",
    "    metadata = pd.read_csv(\"./metadata.csv\").set_index(\"Key\")\n",
    "    met_baseline = pd.read_csv(\"./metabolomics_baseline_2021_2.csv\").set_index(\"Key\")\n",
    "    met_end = pd.read_csv(\"./metabolomics_end_2021_2.csv\").set_index(\"Key\")\n",
    "\n",
    "    # Print raw data shape\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Raw metabolomics baseline rows: {len(met_baseline)}, end rows: {len(met_end)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Raw metabolomics baseline columns: {len(met_baseline.columns)}, end columns: {len(met_end.columns)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tIntersection of columns: {len(set(met_baseline.columns).intersection(set(met_end.columns)))}\"\n",
    "        )\n",
    "\n",
    "    # Subset foods with supplied regex\n",
    "    if foods is not None:\n",
    "        met_baseline = met_baseline.filter(regex=foods, axis=0)\n",
    "        met_end = met_end.filter(regex=foods, axis=0)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No food filter regex supplied!\")\n",
    "        pass\n",
    "\n",
    "    # Split grains if needed\n",
    "    if split_grains:\n",
    "\n",
    "        def update_index(i):\n",
    "            # Ignore all non-grains\n",
    "            if \"Grains\" not in i:\n",
    "                return i\n",
    "            # Map all nograins to nobarley for now\n",
    "            if \"NoGrains\" in i:\n",
    "                return i.replace(\"Grains\", \"Barley\")\n",
    "            # Map Grains --> Barley or Oats\n",
    "            metadata_row = metadata.loc[i]\n",
    "            i = i.split(\".\")\n",
    "            return f\"{i[0]}.{i[1]}.{metadata_row.Treatment2}\"\n",
    "\n",
    "        # Apply index update\n",
    "        met_baseline.index = met_baseline.index.map(update_index)\n",
    "        met_end.index = met_end.index.map(update_index)\n",
    "\n",
    "        # Extract a copy of all \"barley\" (i.e. grains) control\n",
    "        met_baseline_nobarley = met_baseline.filter(like=\"NoBarley\", axis=0).copy()\n",
    "        met_end_nobarley = met_end.filter(like=\"NoBarley\", axis=0).copy()\n",
    "\n",
    "        # Change the copy to oats control\n",
    "        def update_index(i):\n",
    "            return i.replace(\"Barley\", \"Oats\")\n",
    "\n",
    "        # Apply index update\n",
    "        met_baseline_nobarley.index = met_baseline_nobarley.index.map(update_index)\n",
    "        met_end_nobarley.index = met_end_nobarley.index.map(update_index)\n",
    "\n",
    "        # Add copy of grains control to the dataset\n",
    "        met_baseline = pd.concat([met_baseline, met_baseline_nobarley])\n",
    "        met_end = pd.concat([met_end, met_end_nobarley])\n",
    "\n",
    "    # Modify IDs to remove period qualifiers so we can subtract on index\n",
    "    met_baseline.index = met_baseline.index.map(lambda i: i.replace(\".Baseline\", \"\"))\n",
    "    met_end.index = met_end.index.map(lambda i: i.replace(\".End\", \"\"))\n",
    "\n",
    "    return metadata, met_baseline, met_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop/fix mising values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keep features that have proportion of missing values <  p for any food\n",
    "def drop_missing_values(\n",
    "    met_baseline, met_end, metadata, p, split_grains=False, verbose=False\n",
    "):\n",
    "\n",
    "    columns_to_keep_baseline = set()\n",
    "    columns_to_keep_end = set()\n",
    "\n",
    "    foods = set(metadata.Study)\n",
    "    if split_grains:\n",
    "        foods.remove(\"Grains\")\n",
    "        foods.add(\"Barley\")\n",
    "        foods.add(\"Oats\")\n",
    "\n",
    "    for study in foods:\n",
    "        # Select dataset for this study\n",
    "        met_baseline_study = met_baseline.filter(like=study, axis=0)\n",
    "        met_end_study = met_end.filter(like=study, axis=0)\n",
    "\n",
    "        # Compute percent of missing values for the datasets\n",
    "        p_baseline = met_baseline_study.isnull().sum() / len(met_baseline_study)\n",
    "        p_end = met_end_study.isnull().sum() / len(met_end_study)\n",
    "\n",
    "        # Keep all features that have < p percent missing\n",
    "        # i.e. have > p percent features present\n",
    "        p_baseline = p_baseline < p\n",
    "        p_end = p_end < p\n",
    "\n",
    "        # Subset feature list to only include those features\n",
    "        p_baseline = p_baseline.where(lambda a: a).dropna().index\n",
    "        p_end = p_end.where(lambda a: a).dropna().index\n",
    "\n",
    "        # Add column to keep list\n",
    "        columns_to_keep_baseline.update(list(p_baseline))\n",
    "        columns_to_keep_end.update(list(p_end))\n",
    "\n",
    "    # Subset columns\n",
    "    met_baseline = met_baseline[list(columns_to_keep_baseline)]\n",
    "    met_end = met_end[list(columns_to_keep_end)]\n",
    "\n",
    "    # Print results\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Total number of columns after dropping missing (baseline, end) = {(len(columns_to_keep_baseline), len(columns_to_keep_end))}\"\n",
    "        )\n",
    "\n",
    "    return met_baseline, met_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imputes missing values to uniform random values between [0, mm * minimum observed] for every feature\n",
    "def impute_missing_values(met_baseline, met_end, mm, verbose=False):\n",
    "\n",
    "    # Compute per-feature minimums for dataset\n",
    "    met_baseline_feature_mins = np.min(met_baseline, axis=0)\n",
    "    met_baseline_nan_dict = {}\n",
    "    met_end_feature_mins = np.min(met_end, axis=0)\n",
    "    met_end_nan_dict = {}\n",
    "\n",
    "    # Create new datasets that contains random values for each subject for each feature,\n",
    "    # between 0 and mm * the minimum for that feature\n",
    "    for feature, minimum in met_baseline_feature_mins.iteritems():\n",
    "        met_baseline_nan_dict[feature] = np.random.uniform(\n",
    "            low=0, high=mm * minimum, size=len(met_baseline)\n",
    "        )\n",
    "\n",
    "    for feature, minimum in met_end_feature_mins.iteritems():\n",
    "        met_end_nan_dict[feature] = np.random.uniform(\n",
    "            low=0, high=mm * minimum, size=len(met_end)\n",
    "        )\n",
    "\n",
    "    # Update original dataset with new values for any missing entries\n",
    "    # Original values should be preserved\n",
    "    met_baseline_nan = pd.DataFrame(met_baseline_nan_dict)\n",
    "    met_baseline_nan.index = met_baseline.index\n",
    "\n",
    "    met_end_nan = pd.DataFrame(met_end_nan_dict)\n",
    "    met_end_nan.index = met_end.index\n",
    "\n",
    "    met_baseline.update(met_baseline_nan, overwrite=False)\n",
    "    met_end.update(met_end_nan, overwrite=False)\n",
    "\n",
    "    return met_baseline, met_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keeps only columns/subjects available in both datasets, and separates datasets into treatment + control\n",
    "def subset_separate_data(met_baseline, met_end, verbose=False):\n",
    "\n",
    "    # Compute intersection rows\n",
    "    row_idxs = met_baseline.index.intersection(met_end.index)\n",
    "    baseline_index = set(met_baseline.index)\n",
    "    end_index = set(met_end.index)\n",
    "    if verbose:\n",
    "        print(f\"Missing from end: {baseline_index - end_index}\")\n",
    "        print(f\"Missing from baseline: {end_index - baseline_index}\")\n",
    "\n",
    "    # Compute intersection columns\n",
    "    col_idxs = met_baseline.columns.intersection(met_end.columns)\n",
    "\n",
    "    # Subset datasets\n",
    "    met_baseline = met_baseline.loc[row_idxs, col_idxs]\n",
    "    met_end = met_end.loc[row_idxs, col_idxs]\n",
    "    if verbose:\n",
    "        print(f\"Lengths: {(len(met_baseline), len(met_end))}\")\n",
    "    assert len(met_baseline) == len(met_end)\n",
    "\n",
    "    # Separate treatment/control rows\n",
    "    row_idxs_treatment = [idx for idx in row_idxs if \".No\" not in idx]\n",
    "    row_idxs_control = [idx for idx in row_idxs if \".No\" in idx]\n",
    "    assert len(row_idxs_control) + len(row_idxs_treatment) == len(row_idxs)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Remaining rows for (treatment, control, total): {(len(row_idxs_treatment), len(row_idxs_control), len(row_idxs))}\"\n",
    "        )\n",
    "\n",
    "    met_baseline_cont = met_baseline.loc[row_idxs_control, col_idxs]\n",
    "    met_baseline_treat = met_baseline.loc[row_idxs_treatment, col_idxs]\n",
    "    met_end_cont = met_end.loc[row_idxs_control, col_idxs]\n",
    "    met_end_treat = met_end.loc[row_idxs_treatment, col_idxs]\n",
    "\n",
    "    # Extract labels for the treatments\n",
    "    met_treatments = met_baseline_treat.index.map(lambda i: i.split(\".\")[-1])\n",
    "    # Extract labels for the studies\n",
    "    met_studies = met_baseline_cont.index.map(lambda i: i.split(\".\")[-1][2:])\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"\\tTotal number of subjects for (baseline, end) after subsetting = {(len(met_baseline), len(met_end))}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\tTreatment subjects, Control subjects: ({len(met_treatments)}, {len(met_studies)})\"\n",
    "        )\n",
    "\n",
    "        print(f\"Total number of columns after subsetting = {len(col_idxs)}\")\n",
    "\n",
    "    return (\n",
    "        met_baseline,\n",
    "        met_end,\n",
    "        met_baseline_cont,\n",
    "        met_baseline_treat,\n",
    "        met_end_cont,\n",
    "        met_end_treat,\n",
    "        met_treatments,\n",
    "        met_studies,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subtracts two dataframes\n",
    "def subtract_data(met_baseline, met_end):\n",
    "    return met_end - met_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_column_names(df):\n",
    "    df = df.copy()\n",
    "    column_map = {\n",
    "        \"C18:0\": \"10-hydroxystearic acid\",\n",
    "        \"C18:2 (9,12)\": \"Linoleic acid\",\n",
    "        \"C17:0\": \"Margaric acid\",\n",
    "        \"C24:0\": \"Lignoceric acid\",\n",
    "        \"C26:0\": \"Cerotic acid\",\n",
    "        \"C5:0\": \"Valeric acid\",\n",
    "        \"C16:0\": \"Palmitic acid\",\n",
    "        \"C17:0\": \"Margaric acid\",\n",
    "        \"C26:0\": \"Cerotic acid\",\n",
    "        \"C18:1 (9)\": \"Oleic acid\",\n",
    "    }\n",
    "\n",
    "    df.columns = df.columns.map(lambda i: i.capitalize())\n",
    "    df = df.rename(columns=column_map)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_pca(\n",
    "    X,\n",
    "    hue,\n",
    "    title,\n",
    "    plot_evr=False,\n",
    "):\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "\n",
    "    # Scale input\n",
    "    X = QuantileTransformer(n_quantiles=len(X)).fit_transform(X)\n",
    "\n",
    "    # Compute PCA, plot\n",
    "    pca = PCA(n_components=min(20, X.shape[1]), random_state=1)\n",
    "    X_t = pca.fit_transform(X)\n",
    "\n",
    "    if plot_evr:\n",
    "        fig, (ax_pca, ax_evr) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    else:\n",
    "        fig, ax_pca = plt.subplots(1, 1, figsize=(12, 4))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.scatterplot(x=X_t[:, 0], y=X_t[:, 1], hue=hue, ax=ax_pca, s=100)\n",
    "    ax_pca.set_title(f\" {title} PCA\")\n",
    "\n",
    "    # Plot explained variance ratio\n",
    "    if plot_evr:\n",
    "        ax_evr.plot(pca.explained_variance_ratio_)\n",
    "        ax_evr.set_title(\"Explained Variance Ratio\")\n",
    "        ax_evr.set_xlabel(\"PC #\")\n",
    "        ax_evr.set_ylabel(\"Explained Variance Ratio\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def panel_plots(\n",
    "    met_diff_treat,\n",
    "    met_treatments,\n",
    "    met_diff_cont,\n",
    "    met_studies,\n",
    "    group1,  # always in the treatment group\n",
    "    group2,  # could be in the treatment group, or \"Control\"\n",
    "    feature_list,\n",
    "    mapping={},\n",
    "    color_map={\n",
    "        \"Almond\": sns.color_palette()[0],\n",
    "        \"Walnut\": sns.color_palette()[1],\n",
    "        \"Control\": sns.color_palette()[7],\n",
    "    },\n",
    "):\n",
    "\n",
    "    import matplotlib.ticker as ticker\n",
    "    from matplotlib import rc\n",
    "    import glob\n",
    "\n",
    "    assert len(feature_list) >= 1\n",
    "\n",
    "    # Extract group data\n",
    "    group1_data = met_diff_treat[met_treatments == group1].copy()\n",
    "    group1_data[\"group\"] = group1\n",
    "    group2_data = (\n",
    "        met_diff_cont[met_studies == group1].copy()\n",
    "        if group2 == \"Control\"\n",
    "        else met_diff_treat[met_treatments == group2].copy()\n",
    "    )\n",
    "    group2_data[\"group\"] = group2\n",
    "\n",
    "    data = pd.concat([group1_data, group2_data])\n",
    "\n",
    "    # Make plots\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=1, ncols=len(feature_list), figsize=(7.3, 4.5), sharex=\"all\", dpi=300\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\"\", size=22)\n",
    "    for i, (feature, ax) in enumerate(zip(feature_list, axs.flat)):\n",
    "        sns.boxplot(\n",
    "            data=data,\n",
    "            x=\"group\",\n",
    "            y=feature,\n",
    "            linewidth=3,\n",
    "            width=0.6,\n",
    "            ax=ax,\n",
    "            palette=color_map,\n",
    "        )\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set_title(feature if feature not in mapping else mapping[feature])\n",
    "        ax.text(\n",
    "            0.05,\n",
    "            0.92,\n",
    "            \"A\" if i == 0 else \"B\",\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=16,\n",
    "            fontweight=1000,\n",
    "        )\n",
    "        locator = ticker.MaxNLocator(\n",
    "            nbins=3, integer=True, symmetric=True, min_n_ticks=4, prune=\"both\"\n",
    "        )\n",
    "        ax.yaxis.set_major_locator(locator)\n",
    "        ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "        ticks = locator()\n",
    "\n",
    "    fig.suptitle(f\"{group1} vs. {group2}\", size=22)\n",
    "    fig.supxlabel(\"Group\", size=20)\n",
    "    fig.supylabel(\"Δ Relative concentration\", size=20)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.savefig(\n",
    "        f\"panel-boxplot-{group1}-{group2}.svg\", bbox_inches=\"tight\", format=\"svg\"\n",
    "    )\n",
    "    fig.savefig(f\"panel-boxplot-{group1}-{group2}.png\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classification(X, y, title, X_control=None, y_control=None, fast_mode=False):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import LeaveOneOut\n",
    "    from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "    from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "    from sklearn.metrics import classification_report, roc_auc_score\n",
    "    from scikitplot.metrics import plot_confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    # naively assume the output classes are in alphabetical order. if something breaks, look here!\n",
    "    classes = sorted(list(set(y)))\n",
    "\n",
    "    # setup directory\n",
    "    fdir = \"-\".join(c[:3] for c in classes)\n",
    "    os.makedirs(fdir, exist_ok=True)\n",
    "\n",
    "    print(f\"------- {title} -------\")\n",
    "\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [5000 if not fast_mode else 1000],\n",
    "        \"oob_score\": [True],\n",
    "        \"n_jobs\": [-1],\n",
    "        \"random_state\": [1],\n",
    "        \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "        \"min_samples_leaf\": [1, 3, 5],\n",
    "    }\n",
    "\n",
    "    best_rf = None\n",
    "    best_params = None\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        rfc = RandomForestClassifier()\n",
    "        rfc.set_params(**params)\n",
    "\n",
    "        # Perform LOO evaluation for this parameter set\n",
    "        cv_result = cross_validate(\n",
    "            rfc,\n",
    "            X.values,\n",
    "            y,\n",
    "            scoring=None,\n",
    "            cv=LeaveOneOut(),\n",
    "            n_jobs=-1,\n",
    "            return_estimator=True,\n",
    "        )\n",
    "\n",
    "        # Update the best parameters\n",
    "        estimators = cv_result[\"estimator\"]\n",
    "        for estimator in estimators:\n",
    "            if best_rf is None or estimator.oob_score_ > best_rf.oob_score_:\n",
    "                best_rf = estimator\n",
    "                best_params = params\n",
    "        # early exit\n",
    "        if fast_mode:\n",
    "            break\n",
    "\n",
    "    print(\n",
    "        f\"Best params for multi-food classification ({title}) were {best_params}. Fast mode was: {fast_mode}\"\n",
    "    )\n",
    "\n",
    "    # Cross-val predict probabilities using leave one out and our new best parameters\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.set_params(**best_params)\n",
    "\n",
    "    y_proba = cross_val_predict(\n",
    "        rfc, X.values, y, cv=LeaveOneOut(), n_jobs=-1, method=\"predict_proba\"\n",
    "    )\n",
    "\n",
    "    # Convert probs to class scores\n",
    "    y_pred = [classes[score.argmax()] for score in y_proba]\n",
    "\n",
    "    # Try to compute ROC AUC if possible\n",
    "    roc_auc = None\n",
    "    try:\n",
    "        if len(classes) > 2:\n",
    "            roc_auc = roc_auc_score(y, y_proba, multi_class=\"ovr\")\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y, y_proba[:, 1], multi_class=\"ovr\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Couldn't compute ROC AUC score!\")\n",
    "\n",
    "    # Plot results\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    plot_confusion_matrix(y, y_pred, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.title(f\"{title} Treatment\")\n",
    "    plt.show()\n",
    "    print(classification_report(y, y_pred))\n",
    "    if roc_auc:\n",
    "        print(f\"ROC AUC = {roc_auc}\")\n",
    "\n",
    "    # Plot feature importance graph\n",
    "    # rfc.fit(X, y)\n",
    "    best_feature_idxs = np.argsort(best_rf.feature_importances_)[::-1]\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.xlabel(\"Feature #\")\n",
    "    plt.ylabel(\"Importance\")\n",
    "    plt.plot(sorted(best_rf.feature_importances_, reverse=True))\n",
    "    plt.show()\n",
    "    best_features = X.columns[best_feature_idxs[:10]]\n",
    "    print(best_features)\n",
    "\n",
    "    # feautre importances\n",
    "    best_features_list = list(\n",
    "        zip(\n",
    "            [X.columns[idx] for idx in best_feature_idxs],\n",
    "            [best_rf.feature_importances_[idx] for idx in best_feature_idxs],\n",
    "        )\n",
    "    )\n",
    "    with open(f\"{fdir}/{title}-multifood-feature-importances.csv\", \"w\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"feature\", \"importance\"])\n",
    "        for idx in best_feature_idxs:\n",
    "            w.writerow([X.columns[idx], best_rf.feature_importances_[idx]])\n",
    "\n",
    "    # feature means per group\n",
    "    X_gb = X.copy().iloc[:, best_feature_idxs]\n",
    "    X_gb[\"treatment\"] = y\n",
    "    X_gb.groupby(\"treatment\").mean().to_csv(\n",
    "        f\"{fdir}/{title}-multifood-feature-means.csv\"\n",
    "    )\n",
    "    X_gb.groupby(\"treatment\").std().to_csv(f\"{fdir}/{title}-multifood-feature-stds.csv\")\n",
    "\n",
    "    # plot features\n",
    "    for feature in best_features:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        sns.boxplot(\n",
    "            data=X_gb, x=\"treatment\", y=feature, linewidth=2.5, width=0.4, ax=ax\n",
    "        )\n",
    "        ax.set_xlabel(\"Treatment group\")\n",
    "        ax.set_ylabel(\"Relative concentration\")\n",
    "        fig.suptitle(feature, size=22)\n",
    "        fig.savefig(f\"{fdir}/{title}-{feature}-boxplot.png\", bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Control group classification using the best model\n",
    "    if X_control is not None:\n",
    "        y_proba_control = best_rf.predict_proba(X_control.values)\n",
    "        y_pred_control = [classes[score.argmax()] for score in y_proba_control]\n",
    "\n",
    "        roc_auc_control = None\n",
    "        try:\n",
    "            roc_auc_control = roc_auc_score(\n",
    "                y_control, y_proba_control[:, 1], multi_class=\"ovr\"\n",
    "            )\n",
    "        except:\n",
    "            print(\"Couldn't compute ROC AUC score!\")\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        plot_confusion_matrix(y_control, y_pred_control, ax=ax)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"Control Group - {title}\")\n",
    "        plt.show()\n",
    "\n",
    "        print(classification_report(y_control, y_pred_control))\n",
    "        if roc_auc_control:\n",
    "            print(f\"ROC AUC = {roc_auc_control}\")\n",
    "\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    return best_params, best_features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch effect removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_batch_effect(df_cont, cont_labels, df_treat, treat_labels, n=10):\n",
    "    from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "    df_treat_columns = df_treat.columns\n",
    "    df_cont_columns = df_cont.columns\n",
    "    dfs_cont = []\n",
    "    dfs_treat = []\n",
    "\n",
    "    for food in set(treat_labels):\n",
    "        # control data for this study\n",
    "        df_cont_food = df_cont[cont_labels == food]\n",
    "        # treatment data for this study\n",
    "        df_treat_food = df_treat[treat_labels == food]\n",
    "        # center the treatment group on the median of the control\n",
    "        df_treat_food -= df_cont_food.median(0)\n",
    "        # center the control group on their own median\n",
    "        df_cont_food -= df_cont_food.median(0)\n",
    "        # append to main list\n",
    "        dfs_cont.append(df_cont_food)\n",
    "        dfs_treat.append(df_treat_food)\n",
    "\n",
    "    # merge dataframes\n",
    "    df_cont = pd.concat(dfs_cont)\n",
    "    df_treat = pd.concat(dfs_treat)\n",
    "\n",
    "    # generate control basis vectors\n",
    "    def decompose_pca(X, n=n):\n",
    "        U, E, V = np.linalg.svd(X)\n",
    "        return U[:, :n], E[:n], V[:, :n]\n",
    "\n",
    "    # control_basis_vectors = {\"all\": decompose_pca(df_cont, n=2)}\n",
    "    control_basis_vectors = {}\n",
    "    mean_vectors = []\n",
    "    for study in set(cont_labels):\n",
    "        df_cont_food = df_cont[cont_labels == study]\n",
    "        # control_basis_vectors[study] = decompose_pca(df_cont_food, n=3)\n",
    "        mean_vectors.append(df_cont_food.mean(0).values)\n",
    "\n",
    "    control_mean_vectors = decompose_pca(mean_vectors, n=len(mean_vectors))\n",
    "    control_basis_vectors[\"all_mean\"] = control_mean_vectors\n",
    "\n",
    "    # Combines the dictionary of basis vectors into one list\n",
    "    control_basis_vectors = np.concatenate(\n",
    "        [control_basis_vectors[key][2] for key in control_basis_vectors], axis=1\n",
    "    )\n",
    "\n",
    "    control_basis_transformation = (\n",
    "        control_basis_vectors\n",
    "        @ np.linalg.inv((control_basis_vectors.T) @ control_basis_vectors)\n",
    "        @ control_basis_vectors.T\n",
    "    )\n",
    "\n",
    "    # transform treatment\n",
    "    df_treat = (\n",
    "        (np.eye(control_basis_transformation.shape[0]) - control_basis_transformation)\n",
    "        @ (df_treat).T\n",
    "    ).T\n",
    "\n",
    "    # transform control\n",
    "    df_cont = (\n",
    "        (np.eye(control_basis_transformation.shape[0]) - control_basis_transformation)\n",
    "        @ (df_cont).T\n",
    "    ).T\n",
    "\n",
    "    df_treat.columns = df_treat_columns\n",
    "    df_cont.columns = df_cont_columns\n",
    "\n",
    "    return df_cont, df_treat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-food logistic regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def per_food_models(X_treat, y_treat, title, X_control, y_control, fast_mode=False):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import LeaveOneOut\n",
    "    from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "    from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "    from sklearn.metrics import classification_report, roc_auc_score\n",
    "    from scikitplot.metrics import plot_confusion_matrix\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    import numpy as np\n",
    "\n",
    "    foods = sorted(list(set(y_treat)))\n",
    "\n",
    "    # Combine datasets\n",
    "    X = pd.concat([X_treat, X_control])\n",
    "    y_control = \"No\" + y_control\n",
    "    y = list(y_treat) + list(y_control)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Param grid to search for each food\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [5000 if not fast_mode else 1000],\n",
    "        \"oob_score\": [True],\n",
    "        \"n_jobs\": [-1],\n",
    "        \"random_state\": [1],\n",
    "        \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "        \"min_samples_leaf\": [1, 3, 5],\n",
    "    }\n",
    "\n",
    "    # per-food best features\n",
    "    best_features_per_food = {}\n",
    "\n",
    "    for food in foods:\n",
    "        print(f\"------- {title} - {food} -------\")\n",
    "\n",
    "        # make directory\n",
    "        fdir = f\"{food}\"\n",
    "        os.makedirs(fdir, exist_ok=True)\n",
    "\n",
    "        # Extract labels/data for this food\n",
    "        idx = [(food in l) for l in y]\n",
    "        X_food = X.loc[idx]\n",
    "        y_food = y[idx]\n",
    "        print(y_food)\n",
    "\n",
    "        # Naively assume the output classes are in alphabetical order. if something breaks, look here!\n",
    "        classes = sorted(list(set(y_food)))\n",
    "\n",
    "        # Grid search\n",
    "        best_rf = None\n",
    "        best_params = None\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            rfc = RandomForestClassifier()\n",
    "            rfc.set_params(**params)\n",
    "\n",
    "            # Perform LOO evaluation for this parameter set\n",
    "            cv_result = cross_validate(\n",
    "                rfc,\n",
    "                X_food.values,\n",
    "                y_food,\n",
    "                scoring=None,\n",
    "                cv=LeaveOneOut(),\n",
    "                n_jobs=-1,\n",
    "                return_estimator=True,\n",
    "            )\n",
    "\n",
    "            # Update the best parameters\n",
    "            estimators = cv_result[\"estimator\"]\n",
    "            for estimator in estimators:\n",
    "                if best_rf is None or estimator.oob_score_ > best_rf.oob_score_:\n",
    "                    best_rf = estimator\n",
    "                    best_params = params\n",
    "\n",
    "            # early exit\n",
    "            if fast_mode:\n",
    "                break\n",
    "\n",
    "        print(\n",
    "            f\"Best parameters for {food} single-food model were {best_params}. Fast mode is {('en' if fast_mode else 'dis') + 'abled'}\"\n",
    "        )\n",
    "\n",
    "        # Cross-val predict probabilities using leave one out and our new best parameters\n",
    "        rfc = RandomForestClassifier()\n",
    "        rfc.set_params(**best_params)\n",
    "        y_proba = cross_val_predict(\n",
    "            rfc,\n",
    "            X_food.values,\n",
    "            y_food,\n",
    "            cv=LeaveOneOut(),\n",
    "            n_jobs=-1,\n",
    "            method=\"predict_proba\",\n",
    "        )\n",
    "\n",
    "        # Convert probs to class scores\n",
    "        y_pred = [classes[score.argmax()] for score in y_proba]\n",
    "\n",
    "        # Try to compute ROC AUC if possible\n",
    "        roc_auc = None\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_food, y_proba[:, 1])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Couldn't compute ROC AUC score!\")\n",
    "\n",
    "        # Plot results\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        plot_confusion_matrix(y_food, y_pred, ax=ax)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f\"{food} {title} Treatment\")\n",
    "        plt.show()\n",
    "        print(classification_report(y_food, y_pred))\n",
    "        if roc_auc:\n",
    "            print(f\"{food} ROC AUC = {roc_auc}\")\n",
    "\n",
    "        # Plot feature importance graph\n",
    "        best_feature_idxs = np.argsort(best_rf.feature_importances_)[::-1]\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.title(f\"{food} Feature Importances\")\n",
    "        plt.xlabel(\"Feature #\")\n",
    "        plt.ylabel(\"Importance\")\n",
    "        plt.plot(sorted(best_rf.feature_importances_, reverse=True))\n",
    "        plt.show()\n",
    "        best_features = X_food.columns[best_feature_idxs[:10]]\n",
    "        print(best_features)\n",
    "\n",
    "        # feature means per group write-out\n",
    "        X_food_gb = X_food.copy().iloc[:, best_feature_idxs]\n",
    "        X_food_gb[\"group\"] = list(map(lambda i: \"Control\" if \"No\" in i else i, y_food))\n",
    "        X_food_gb.groupby(\"group\").mean().to_csv(\n",
    "            f\"{fdir}/{title}-{food}-feature-means.csv\"\n",
    "        )\n",
    "        X_food_gb.groupby(\"group\").std().to_csv(\n",
    "            f\"{fdir}/{title}-{food}-feature-stds.csv\"\n",
    "        )\n",
    "\n",
    "        # Feautre importances write out + figure generation\n",
    "        best_features_list = list(\n",
    "            zip(\n",
    "                [X_food.columns[idx] for idx in best_feature_idxs],\n",
    "                [best_rf.feature_importances_[idx] for idx in best_feature_idxs],\n",
    "            )\n",
    "        )\n",
    "        best_features_per_food[food] = best_features_list\n",
    "        with open(f\"{fdir}/{title}-{food}-feature-importances.csv\", \"w\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"feature\", \"importance\"])\n",
    "            for idx in best_feature_idxs:\n",
    "                w.writerow([X_food.columns[idx], best_rf.feature_importances_[idx]])\n",
    "\n",
    "        # plot features\n",
    "        for feature in best_features:\n",
    "            fig, ax = plt.subplots(figsize=(5, 5))\n",
    "            sns.boxplot(\n",
    "                data=X_food_gb, x=\"group\", y=feature, linewidth=2.5, width=0.4, ax=ax\n",
    "            )\n",
    "            ax.set_xlabel(\"Treatment group\")\n",
    "            ax.set_ylabel(\"Relative concentration\")\n",
    "            fig.suptitle(feature, size=22)\n",
    "            fig.savefig(\n",
    "                f\"{fdir}/{title}-{food}-{feature}-boxplot.png\", bbox_inches=\"tight\"\n",
    "            )\n",
    "            plt.close(fig)\n",
    "\n",
    "    return best_features_per_food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(single_foods=None, multi_foods=None, fast_mode=False):\n",
    "\n",
    "    if fast_mode:\n",
    "        print(\"Warning: Fast mode is enabled!\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    metadata, met_baseline, met_end = load_data(foods=single_foods, split_grains=True)\n",
    "\n",
    "    # 2. Drop missing values\n",
    "    met_baseline, met_end = drop_missing_values(met_baseline, met_end, metadata, p=0.2)\n",
    "\n",
    "    # 3. Impute missing values\n",
    "    met_baseline, met_end = impute_missing_values(met_baseline, met_end, mm=0.25)\n",
    "\n",
    "    # 4. Decompose dataset\n",
    "    (\n",
    "        met_baseline,\n",
    "        met_end,\n",
    "        met_baseline_cont,\n",
    "        met_baseline_treat,\n",
    "        met_end_cont,\n",
    "        met_end_treat,\n",
    "        met_treatments,\n",
    "        met_studies,\n",
    "    ) = subset_separate_data(met_baseline, met_end)\n",
    "\n",
    "    # 5. Subtract datasets\n",
    "    met_diff_treat = subtract_data(met_baseline_treat, met_end_treat)\n",
    "    met_diff_cont = subtract_data(met_baseline_cont, met_end_cont)\n",
    "\n",
    "    # 6. Plot PCA plots\n",
    "    # plot_pca(met_baseline_treat, met_treatments, \"Treatment Baseline\")\n",
    "    # plot_pca(met_baseline, met_baseline.index.map(lambda i: i.split(\".\")[-1]), \"All Baseline\")\n",
    "    # plot_pca(met_diff_treat, met_treatments, \"Treatment Difference\")\n",
    "    # plot_pca(met_diff_cont, met_studies, \"Control Difference\")\n",
    "    # plot_pca(met_end_treat, met_treatments, \"Treatment End\")\n",
    "\n",
    "    # 7. Remove batch effect\n",
    "    # met_diff_cont_nc, met_diff_treat_nc = remove_batch_effect(\n",
    "    #    met_diff_cont, met_studies, met_diff_treat, met_treatments\n",
    "    # )\n",
    "\n",
    "    # plot_pca(\n",
    "    #    met_diff_treat_nc, met_treatments, \"Treatment Difference - Batch Effect Removed\"\n",
    "    # )\n",
    "\n",
    "    # 6. Panel plots for specific foods\n",
    "    if not fast_mode:\n",
    "        panel_plots(\n",
    "            met_diff_treat,\n",
    "            met_treatments,\n",
    "            met_diff_cont,\n",
    "            met_studies,\n",
    "            \"Almond\",\n",
    "            \"Control\",\n",
    "            [\"C18:1 (9)\", \"C18:2 (9,12)\"],\n",
    "            mapping={\n",
    "                \"C18:1 (9)\": \"10-hydroxystearic acid\",\n",
    "                \"C18:2 (9,12)\": \"Linoleic acid\",\n",
    "                \"Tocopherol, a\": r\"α-tocopherol\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        panel_plots(\n",
    "            met_diff_treat,\n",
    "            met_treatments,\n",
    "            met_diff_cont,\n",
    "            met_studies,\n",
    "            \"Walnut\",\n",
    "            \"Control\",\n",
    "            [\"5-hydroxyindole-3-acetic acid\", \"URIC ACID\"],\n",
    "            mapping={\n",
    "                \"5-hydroxyindole-3-acetic acid\": \"5-HIAA\",\n",
    "                \"URIC ACID\": \"Uric acid\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        panel_plots(\n",
    "            met_diff_treat,\n",
    "            met_treatments,\n",
    "            met_diff_cont,\n",
    "            met_studies,\n",
    "            \"Almond\",\n",
    "            \"Walnut\",\n",
    "            [\"5-hydroxyindole-3-acetic acid\", \"Tocopherol, a\"],\n",
    "            mapping={\n",
    "                \"5-hydroxyindole-3-acetic acid\": \"5-HIAA\",\n",
    "                \"Tocopherol, a\": \"α-tocopherol\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # 8. Per-food models\n",
    "    best_features_per_food = per_food_models(\n",
    "        met_diff_treat,\n",
    "        met_treatments,\n",
    "        \"Difference\",\n",
    "        X_control=met_diff_cont,\n",
    "        y_control=met_studies,\n",
    "        fast_mode=fast_mode,\n",
    "    )\n",
    "\n",
    "    # 9. Multi-food models classification\n",
    "    if multi_foods is not None:\n",
    "        met_diff_treat_subset = met_diff_treat[\n",
    "            [t in multi_foods for t in met_treatments]\n",
    "        ]\n",
    "        met_treatments_subset = [t for t in met_treatments if t in multi_foods]\n",
    "        met_diff_cont_subset = met_diff_cont[[t in multi_foods for t in met_studies]]\n",
    "        met_studies_subset = [t for t in met_studies if t in multi_foods]\n",
    "    else:\n",
    "        met_diff_treat_subset = met_diff_treat.copy()\n",
    "        met_treatments_subset = met_treatments\n",
    "        met_diff_cont_subset = met_diff_cont.copy()\n",
    "        met_studies_subset = met_studies\n",
    "\n",
    "    # Difference\n",
    "    _, best_features_multi = classification(\n",
    "        met_diff_treat_subset,\n",
    "        met_treatments_subset,\n",
    "        \"Difference\",\n",
    "        X_control=met_diff_cont_subset,\n",
    "        y_control=met_studies_subset,\n",
    "        fast_mode=fast_mode,\n",
    "    )\n",
    "\n",
    "    # classification(\n",
    "    #    met_diff_treat_nc,\n",
    "    #    met_treatments,\n",
    "    #    \"Difference (Batch Effect Removed)\",\n",
    "    #    X_control=met_diff_cont_nc,\n",
    "    #    y_control=met_studies,\n",
    "    # )\n",
    "\n",
    "    return (\n",
    "        met_baseline,\n",
    "        met_end,\n",
    "        met_baseline_cont,\n",
    "        met_baseline_treat,\n",
    "        met_end_cont,\n",
    "        met_end_treat,\n",
    "        met_diff_cont,\n",
    "        met_diff_treat,\n",
    "        met_treatments,\n",
    "        met_studies,\n",
    "        best_features_per_food,\n",
    "        best_features_multi,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    met_baseline,\n",
    "    met_end,\n",
    "    met_baseline_cont,\n",
    "    met_baseline_treat,\n",
    "    met_end_cont,\n",
    "    met_end_treat,\n",
    "    met_diff_cont,\n",
    "    met_diff_treat,\n",
    "    met_treatments,\n",
    "    met_studies,\n",
    "    best_features_per_food,\n",
    "    best_features_multi,\n",
    ") = pipeline(single_foods=None, multi_foods=[\"Almond\", \"Walnut\"], fast_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_features(\n",
    "    met_diff_treat,\n",
    "    met_treatments,\n",
    "    met_diff_cont,\n",
    "    met_studies,\n",
    "    best_features_per_food,\n",
    "    best_features_multi,\n",
    "    show_all=False,\n",
    "):\n",
    "    from kneed import KneeLocator\n",
    "\n",
    "    sns.set_theme()\n",
    "    # TODO  fix filtering so we don't have to do this, and can just run Almond|Walnut single food models with the same feature set\n",
    "    foods = sorted(list(best_features_per_food.keys()))\n",
    "    foods = [\"Almond\", \"Walnut\"]\n",
    "\n",
    "    # Plot feature importance graphs for selection\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=1, ncols=len(foods) + 1, figsize=(15, 5), sharex=False, sharey=False\n",
    "    )\n",
    "    axs = axs.flat\n",
    "    for food, ax in zip(foods, axs):\n",
    "        feature_imps = list(map(lambda i: i[1], best_features_per_food[food]))\n",
    "        kneedle = KneeLocator(\n",
    "            np.arange(len(feature_imps)),\n",
    "            feature_imps,\n",
    "            S=1.0,\n",
    "            curve=\"convex\",\n",
    "            direction=\"decreasing\",\n",
    "        )\n",
    "        elbow = kneedle.elbow\n",
    "\n",
    "        ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "        sns.lineplot(data=feature_imps[: elbow * 2], ax=ax)\n",
    "        ax.axvline(x=elbow, linestyle=\"dotted\", color=\"grey\")\n",
    "        ax.set_title(food)\n",
    "\n",
    "    feature_imps = list(map(lambda i: i[1], best_features_multi))\n",
    "    kneedle = KneeLocator(\n",
    "        np.arange(len(feature_imps)),\n",
    "        feature_imps,\n",
    "        S=1.0,\n",
    "        curve=\"convex\",\n",
    "        direction=\"decreasing\",\n",
    "    )\n",
    "    elbow = kneedle.elbow\n",
    "    sns.lineplot(data=feature_imps[: elbow * 2], ax=axs[-1])\n",
    "    axs[-1].axvline(x=elbow, linestyle=\"dotted\", color=\"grey\")\n",
    "    axs[-1].set_title(\"Multi-food\")\n",
    "    axs[-1].xaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Plot correlation between single-food features and multi-food features\n",
    "\n",
    "    # Get correlation matrix\n",
    "\n",
    "    met_diff_treat_foods = met_diff_treat.filter(regex=\"|\".join(foods), axis=0)\n",
    "    met_diff_cont_foods = met_diff_cont.filter(regex=\"|\".join(foods), axis=0)\n",
    "    met_diff_foods = pd.concat([met_diff_treat_foods, met_diff_cont_foods])\n",
    "    corr = met_diff_foods.corr(method=\"spearman\")\n",
    "\n",
    "    # Rows = single food features\n",
    "    # Columns = multi food features\n",
    "    corr_rows_per_food = {}\n",
    "    for food in foods:\n",
    "        if food == \"Almond\":\n",
    "            cutoff = 5\n",
    "        elif food == \"Walnut\":\n",
    "            cutoff = 5\n",
    "        if show_all:\n",
    "            cutoff = None\n",
    "        top_features = list(map(lambda i: i[0], best_features_per_food[food][:cutoff]))\n",
    "        if show_all:\n",
    "            print(f\"{food} has {len(best_features_per_food[food])} features\")\n",
    "        corr_rows_per_food[food] = corr.filter(items=top_features, axis=0)\n",
    "\n",
    "    top_features_names_multi = list(map(lambda i: i[0], best_features_multi))[\n",
    "        : 5 if not show_all else None\n",
    "    ]\n",
    "    corr2 = pd.concat(list(corr_rows_per_food.values()))\n",
    "    corr2 = corr2.filter(items=top_features_names_multi)\n",
    "    corr2.to_csv(f\"correlation-{','.join(foods)}-vs-{'-'.join(foods)}.csv\")\n",
    "\n",
    "    mask = np.triu(np.ones_like(corr2, dtype=bool))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(\n",
    "        corr2,\n",
    "        cmap=cmap,\n",
    "        # vmax=1,\n",
    "        # mask=mask,\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.5},\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(\"Multi-food features\")\n",
    "    ax.set_ylabel(\"Single-food features\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return corr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlate_features(\n",
    "    met_diff_treat,\n",
    "    met_treatments,\n",
    "    met_diff_cont,\n",
    "    met_studies,\n",
    "    best_features_per_food,\n",
    "    best_features_multi,\n",
    "    show_all=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def descriptive(met_treatments):\n",
    "\n",
    "    # from mne.stats import fdr_correction\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "    top_features = {}\n",
    "\n",
    "    for food in {\n",
    "        \"Almond\",\n",
    "        \"Avocado\",\n",
    "        \"Barley\",\n",
    "        \"Broccoli\",\n",
    "        \"Oats\",\n",
    "        \"Walnut\",\n",
    "    }:  # set(met_treatments):\n",
    "\n",
    "        # 1. Load Data\n",
    "        if food in {\"Oats\", \"Barley\"}:\n",
    "            metadata, met_baseline, met_end = load_data(foods=\"Grains\", split_grains=True)\n",
    "            met_baseline = met_baseline.filter(like=food, axis=0)\n",
    "            met_end = met_end.filter(like=food, axis=0)\n",
    "        else:\n",
    "            metadata, met_baseline, met_end = load_data(foods=food, split_grains=True)\n",
    "\n",
    "        # 2. Drop missing values\n",
    "        met_baseline, met_end = drop_missing_values(\n",
    "            met_baseline, met_end, metadata, p=0.2, split_grains=True\n",
    "        )\n",
    "\n",
    "        # 3. Impute missing values\n",
    "        # met_baseline, met_end = impute_missing_values(met_baseline, met_end, mm=0.25)\n",
    "\n",
    "        # 4. Decompose dataset\n",
    "        (\n",
    "            met_baseline,\n",
    "            met_end,\n",
    "            met_baseline_cont,\n",
    "            met_baseline_treat,\n",
    "            met_end_cont,\n",
    "            met_end_treat,\n",
    "            met_treatments,\n",
    "            met_studies,\n",
    "        ) = subset_separate_data(met_baseline, met_end)\n",
    "\n",
    "        # 5. Subtract datasets\n",
    "        # met_diff_treat = subtract_data(met_baseline_treat, met_end_treat)\n",
    "        # met_diff_cont = subtract_data(met_baseline_cont, met_end_cont)\n",
    "        met_diff_treat = np.log(met_end_treat) - np.log(met_baseline_treat)\n",
    "        met_diff_cont = np.log(met_end_cont) - np.log(met_baseline_cont)\n",
    "\n",
    "        # return [\n",
    "        #    scipy.stats.shapiro(met_diff_treat[x])[1] for x in met_diff_treat.columns\n",
    "        # ]\n",
    "\n",
    "        s, p = scipy.stats.ttest_ind(\n",
    "            met_diff_treat,\n",
    "            met_diff_cont,\n",
    "            axis=0,\n",
    "            nan_policy=\"omit\",\n",
    "        )\n",
    "\n",
    "        _, p, _, _, = multipletests(\n",
    "            p, alpha=0.05, method=\"fdr_bh\", is_sorted=False, returnsorted=False\n",
    "        )\n",
    "        idx = p < 0.05\n",
    "\n",
    "        top_features[food] = met_diff_treat.columns[idx]\n",
    "\n",
    "    top_almond = set(top_features[\"Almond\"])\n",
    "    top_walnut = set(top_features[\"Walnut\"])\n",
    "    intersect = top_almond.intersection(top_walnut)\n",
    "\n",
    "    print(f\"Almond ({len(top_almond)}) = {top_almond}\")\n",
    "    print(f\"Walnut ({len(top_walnut)}) = {top_walnut}\")\n",
    "    print(f\"Intersection = ({len(intersect)}) = {intersect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive(met_treatments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foldchangemap():\n",
    "\n",
    "    aminos = [\n",
    "        \"Cysteine\",\n",
    "        \"Glycine\",\n",
    "        \"Histidine\",\n",
    "        \"alanine\",\n",
    "        \"arginine\",\n",
    "        \"asparagine\",\n",
    "        \"aspartic acid\",\n",
    "        \"glutamic acid\",\n",
    "        \"glutamine \",\n",
    "        \"leucine\",\n",
    "        \"isoleucine\",\n",
    "        \"Lysine\",\n",
    "        \"methionine\",\n",
    "        \"phenylalanine\",\n",
    "        \"proline\",\n",
    "        \"serine\",\n",
    "        \"threonine\",\n",
    "        \"tyrosine\",\n",
    "        \"valine\",\n",
    "        \"ornithine\",\n",
    "        \"tryptophan\",\n",
    "    ]\n",
    "\n",
    "    np.seterr(all=\"ignore\")\n",
    "\n",
    "    met_names = pd.read_excel(\"./Metabolites classification.xlsx\").set_index(\"Name\")\n",
    "\n",
    "    other_dfs = []\n",
    "\n",
    "    for superclass in set(met_names.Superclass):\n",
    "\n",
    "        columns = met_names[met_names.Superclass == superclass]\n",
    "\n",
    "        fcs = {}\n",
    "\n",
    "        for study in {\n",
    "            \"Almond\",\n",
    "            \"Avocado\",\n",
    "            \"Broccoli\",\n",
    "            \"Barley\",\n",
    "            \"Oats\",\n",
    "            \"Walnut\",\n",
    "        }:\n",
    "\n",
    "            # 1. Load Data\n",
    "            met_baseline, met_end = None, None\n",
    "            if study in {\"Oats\", \"Barley\"}:\n",
    "                metadata, met_baseline, met_end = load_data(foods=\"Grains\", split_grains=True)\n",
    "                met_baseline = met_baseline.filter(like=study, axis=0)\n",
    "                met_end = met_end.filter(like=study, axis=0)\n",
    "            else:\n",
    "                metadata, met_baseline, met_end = load_data(foods=study, split_grains=True)\n",
    "\n",
    "            # 2. Drop missing values\n",
    "            met_baseline, met_end = drop_missing_values(\n",
    "                met_baseline, met_end, metadata, p=0.5, split_grains=True\n",
    "            )\n",
    "\n",
    "            # 3. Impute missing values\n",
    "            met_baseline, met_end = impute_missing_values(\n",
    "                met_baseline, met_end, mm=0.25\n",
    "            )\n",
    "\n",
    "            # 4. Decompose dataset\n",
    "            (\n",
    "                met_baseline,\n",
    "                met_end,\n",
    "                met_baseline_cont,\n",
    "                met_baseline_treat,\n",
    "                met_end_cont,\n",
    "                met_end_treat,\n",
    "                met_treatments,\n",
    "                met_studies,\n",
    "            ) = subset_separate_data(met_baseline, met_end)\n",
    "\n",
    "            # 5. Subtract data\n",
    "            diff_treat = np.log2(1 + met_end_treat) - np.log2(1 + met_baseline_treat)\n",
    "            diff_cont = np.log2(1 + met_end_cont) - np.log2(1 + met_baseline_cont)\n",
    "\n",
    "            # Fix columns\n",
    "            # met_diff_treat = fix_column_names(met_diff_treat)\n",
    "            # met_diff_cont = fix_column_names(met_diff_cont)\n",
    "\n",
    "            mean_treat = diff_treat.mean(axis=0)\n",
    "            mean_cont = diff_cont.mean(axis=0)\n",
    "\n",
    "            foldchange = mean_treat - mean_cont\n",
    "\n",
    "            # foldchange = np.log2(foldchange)\n",
    "\n",
    "            fcs[study] = foldchange\n",
    "\n",
    "        df = pd.DataFrame(fcs).fillna(0)\n",
    "        df = df.sort_index(axis=1)\n",
    "\n",
    "        c = columns.index.tolist()\n",
    "        c = [i.strip() for i in c]\n",
    "        c = list(filter(lambda i: i in df.index, c))\n",
    "\n",
    "        ######\n",
    "        def func(i):\n",
    "            a = (\n",
    "                i in aminos\n",
    "                or i.lower() in aminos\n",
    "                or i.upper() in aminos\n",
    "                or i.capitalize() in aminos\n",
    "            )\n",
    "            return a\n",
    "\n",
    "        if superclass == \"Organic acids and derivates\":\n",
    "            c = list(\n",
    "                filter(\n",
    "                    func,\n",
    "                    c,\n",
    "                )\n",
    "            )\n",
    "        ######\n",
    "\n",
    "        df = df.loc[c, :]\n",
    "\n",
    "        if len(df) <= 5:\n",
    "            print(f\"{superclass} only has {df.index}!\")\n",
    "            other_dfs.append(df)\n",
    "            continue\n",
    "\n",
    "        # Generate a custom diverging colormap\n",
    "        cmap = sns.diverging_palette(10, 150, l=35, as_cmap=True)\n",
    "\n",
    "        # Draw the heatmap with the mask and correct aspect ratio\n",
    "        df = fix_column_names(df.T).T\n",
    "        minmax = max(np.abs(df.min().min()), np.abs(df.max().max()))\n",
    "        minmax = int(np.ceil(minmax))\n",
    "        g = sns.clustermap(\n",
    "            df,\n",
    "            cmap=cmap,\n",
    "            vmax=minmax,\n",
    "            vmin=-minmax,\n",
    "            center=0,\n",
    "            linewidths=0.1,\n",
    "            figsize=(10, 10),\n",
    "            fmt=\".1f\",\n",
    "            cbar_kws={\n",
    "                # \"orientation\": \"horizontal\",\n",
    "                \"label\": r\"$log_2$(FC Ratio)\",\n",
    "            },\n",
    "            dendrogram_ratio=(0.1, 0.05),\n",
    "            cbar_pos=(1.2, 0.85, 0.05, 0.1),\n",
    "            yticklabels=True,\n",
    "        )\n",
    "        g.ax_heatmap.set_xticklabels(\n",
    "            g.ax_heatmap.get_xmajorticklabels(), fontsize=18, rotation=45\n",
    "        )\n",
    "        g.ax_heatmap.set_yticklabels(\n",
    "            g.ax_heatmap.get_ymajorticklabels(), fontsize=14, rotation=0\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.gcf(), plt.gca()\n",
    "        # plt.show()\n",
    "        plt.savefig(f\"{superclass}.png\", bbox_inches=\"tight\")\n",
    "\n",
    "    df = pd.concat(other_dfs)\n",
    "    df = fix_column_names(df.T).T\n",
    "    minmax = max(np.abs(df.min().min()), np.abs(df.max().max()))\n",
    "    minmax = int(np.ceil(minmax))\n",
    "    sns.clustermap(\n",
    "        df,\n",
    "        cmap=cmap,\n",
    "        vmax=minmax,\n",
    "        vmin=-minmax,\n",
    "        center=0,\n",
    "        linewidths=0.1,\n",
    "        figsize=(8, 5),\n",
    "        fmt=\".1f\",\n",
    "        cbar_kws={\n",
    "            # \"orientation\": \"horizontal\",\n",
    "            \"label\": r\"$log_2$(FC Ratio)\",\n",
    "        },\n",
    "        dendrogram_ratio=(0.1, 0.05),\n",
    "        cbar_pos=(1.2, 0.85, 0.05, 0.1),\n",
    "        yticklabels=True,\n",
    "    )\n",
    "    fig, ax = plt.gcf(), plt.gca()\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"other.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldchangemap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate2():\n",
    "\n",
    "    # 1. Load Data\n",
    "    metadata, met_baseline, met_end = load_data(foods=None, split_grains=False)\n",
    "\n",
    "    # 2. Drop missing values\n",
    "    met_baseline, met_end = drop_missing_values(met_baseline, met_end, metadata, p=0.2)\n",
    "\n",
    "    # 3. Impute missing values\n",
    "    # met_baseline, met_end = impute_missing_values(met_baseline, met_end, mm=0.25)\n",
    "\n",
    "    # 4. Decompose dataset\n",
    "    (\n",
    "        met_baseline,\n",
    "        met_end,\n",
    "        met_baseline_cont,\n",
    "        met_baseline_treat,\n",
    "        met_end_cont,\n",
    "        met_end_treat,\n",
    "        met_treatments,\n",
    "        met_studies,\n",
    "    ) = subset_separate_data(met_baseline, met_end)\n",
    "\n",
    "    column_map = {\n",
    "        \"C18:0\": \"10-hydroxystearic acid\",\n",
    "        \"C18:2 (9,12)\": \"Linoleic acid\",\n",
    "        \"C17:0\": \"Margaric acid\",\n",
    "        \"C24:0\": \"Lignoceric acid\",\n",
    "        \"C26:0\": \"Cerotic acid\",\n",
    "        \"C5:0\": \"Valeric acid\",\n",
    "        \"C16:0\": \"Palmitic acid\",\n",
    "        \"C17:0\": \"Margaric acid\",\n",
    "        \"C26:0\": \"Cerotic acid\",\n",
    "        \"C18:1 (9)\": \"Oleic acid\",\n",
    "    }\n",
    "\n",
    "    met = pd.concat([met_baseline, met_end]).filter(like=\"No\", axis=0)\n",
    "    met.columns = met.columns.map(lambda i: i.capitalize())\n",
    "    assert all([key in met.columns for key in column_map.keys()])\n",
    "    met = met.rename(columns=column_map)\n",
    "\n",
    "    print(len(met_baseline.columns))\n",
    "\n",
    "    studies = set(met.index.map(lambda i: i.split(\".\")[-1]))\n",
    "\n",
    "    corrs = []\n",
    "    factors = []\n",
    "    for study in studies:\n",
    "        met_s = met.filter(like=study, axis=0)\n",
    "        corr_s = met.corr(method=\"spearman\")\n",
    "        corrs.append(corr_s)\n",
    "        factors.append(len(met_s) / len(met))\n",
    "\n",
    "    corr = corrs[0] * factors[0]\n",
    "    for i in range(1, len(corrs)):\n",
    "        corr = corr + corrs[i] * factors[i]\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    # fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "    mask = np.tril(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    g = sns.clustermap(\n",
    "        corr,\n",
    "        cmap=cmap,\n",
    "        vmax=1,\n",
    "        vmin=-1,\n",
    "        center=0,\n",
    "        linewidths=0.1,\n",
    "        figsize=(30, 30),\n",
    "        cbar_kws={\n",
    "            # \"orientation\": \"horizontal\",\n",
    "            # \"label\": r\"$log_2$ FC\",\n",
    "        },\n",
    "        dendrogram_ratio=(0.1, 0.05),\n",
    "        cbar_pos=(1.2, 0.85, 0.05, 0.1),\n",
    "        yticklabels=True,\n",
    "        xticklabels=True,\n",
    "    )\n",
    "    # ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize=8.5, rotation=90)\n",
    "    # ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize=8.5, rotation=0)\n",
    "    g.ax_heatmap.set_xticklabels(\n",
    "        g.ax_heatmap.get_xmajorticklabels(), fontsize=12, rotation=90\n",
    "    )\n",
    "    g.ax_heatmap.set_yticklabels(\n",
    "        g.ax_heatmap.get_ymajorticklabels(), fontsize=12, rotation=0\n",
    "    )\n",
    "    plt.savefig(\"correlation_heatmap.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlate2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
